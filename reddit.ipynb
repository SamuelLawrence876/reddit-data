{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddf0ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import praw\n",
    "import time\n",
    "import pickle\n",
    "# from dotenv import load_dotenv\n",
    "from plotnine import *\n",
    "from praw.models import MoreComments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da78b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"SamTheIceCreamMan\"\n",
    "password = \"Samernator1\"\n",
    "id = \"U9EagV0j3qUSbw\"\n",
    "secret = \"Su_o5xTzvs4RKygGOBakQFutxw49-A\"\n",
    "user_agent = \"scrapermaster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13ab0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=id,\n",
    "                     client_secret=secret,\n",
    "                     password=password, user_agent=user_agent,\n",
    "                     username=username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29bc32ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_limit = 150\n",
    "n_top = 50\n",
    "n_old = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5553533",
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = [\"wallstreetbets\", \"stocks\", \"investing\", \"CyptoCurrency\", \"StockMarket\", \"pennystocks\",\"WallStreetbetsELITE\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb6c72dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_info(post, sub_name):\n",
    "    info = {\n",
    "        'sub': sub_name,\n",
    "        'post_id': post.id,\n",
    "        'title': post.title,\n",
    "        'comments': post.num_comments,\n",
    "        'score': post.score,\n",
    "        'created_utc': post.created_utc\n",
    "    }\n",
    "\n",
    "    return info\n",
    "\n",
    "def get_comment_info(comment, sort):    \n",
    "    info = {\n",
    "        #'sub': comment.subreddit.title,\n",
    "        'post_id': comment.submission.id,\n",
    "        'comment_id': comment.id,\n",
    "        'created_utc': comment.created_utc,\n",
    "        'score': comment.score,\n",
    "        'top_level': comment.parent_id.startswith('t3_'),\n",
    "        'sort': sort}\n",
    "\n",
    "    return info\n",
    "\n",
    "def get_comments(post, sort, n):\n",
    "    post.comment_sort = sort\n",
    "    post.comment_limit = n\n",
    "    #post.comments.replace_more(limit=0)\n",
    "    \n",
    "    return [get_comment_info(c, sort) for c in post.comments.list() if not isinstance(c, MoreComments)]\n",
    "\n",
    "\n",
    "def save_cmnt_df(cmnt_list, sub_name, n, perc):\n",
    "    df_cmnt = pd.DataFrame(cmnt_list)\n",
    "    df = df_cmnt.merge(pd.DataFrame(post_list), on='post_id')\n",
    "    df.columns = ['post_id', 'comment_id', 'comment_created_utc', 'comment_score', 'top_level',\n",
    "                  'sort', 'sub', 'post_title', 'comments_n', 'post_score', 'post_created_utc']\n",
    "    df = df.sort_values(['post_id', 'comment_created_utc', 'sort']).reset_index(drop=True)\n",
    "    df['in_old'] = df['comment_id'].isin(df.loc[df['sort']=='old', 'comment_id'].unique())\n",
    "    df = df.drop_duplicates(subset=['post_id', 'comment_id'], keep='last')\n",
    "    df['t_delta_min'] = (df['comment_created_utc'] - df['post_created_utc'])/60\n",
    "    df['comment_score_pct'] = df.groupby(['post_id'], as_index=False)['comment_score'].transform(lambda x: x/x.sum())\n",
    "    df['nth'] = df.groupby('post_id')['comment_id'].transform(lambda x: range(len(x)))\n",
    "    df.to_csv(f'./data/n-{n}_perc-{perc}_{sub_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db09c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wallstreetbets\n",
      "stocks\n",
      "investing\n",
      "CyptoCurrency\n",
      "StockMarket\n",
      "pennystocks\n"
     ]
    }
   ],
   "source": [
    "for sub_name in subs:\n",
    "    print(sub_name)\n",
    "    post_list, top_list, old_list = [], [], []\n",
    "    sub = reddit.subreddit(sub_name)\n",
    "    \n",
    "    posts = sub.top(limit=sub_limit)\n",
    "    #posts = [p for p in posts if p.num_comments > n_top*10]\n",
    "    for post in posts:\n",
    "        post_list.append(get_post_info(post, sub_name))\n",
    "        top_list.extend(get_comments(post, sort='top', n=n_top))\n",
    "    \n",
    "    posts = sub.top(limit=sub_limit)\n",
    "    for post in posts:\n",
    "        old_list.extend(get_comments(post, sort='old', n=n_old))\n",
    "    \n",
    "    pickle.dump(post_list, open(f'./data/posts_{sub_name}.pickle', 'wb'))\n",
    "    pickle.dump(top_list, open(f'./data/top_{sub_name}.pickle', 'wb'))\n",
    "    pickle.dump(old_list, open(f'./data/old_{sub_name}.pickle', 'wb'))\n",
    "    #save_cmnt_df(post_list, cmnt_list, sub_name, n, perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7229aafb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/posts_wallstreetbets.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c05e70256ec3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[0mdf_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlists_to_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_top\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msub_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comments_n'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-c05e70256ec3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[0mdf_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlists_to_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_top\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msub_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comments_n'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-c05e70256ec3>\u001b[0m in \u001b[0;36munpickle\u001b[1;34m(sub_name)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mpost_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'./data/posts_{sub_name}.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mtop_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'./data/top_{sub_name}.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mold_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'./data/old_{sub_name}.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/posts_wallstreetbets.pickle'"
     ]
    }
   ],
   "source": [
    "\n",
    "def interpolate_n(x):\n",
    "    mean_rate = x.iloc[int(len(x)*0.75):-1]['rate'].mean()\n",
    "    x.loc[x['in_old']==False, 'nth'] = x.loc[x['in_old']==False, 't_delta_min']*mean_rate\n",
    "    \n",
    "    return x\n",
    "\n",
    "def in_oldest(x):\n",
    "    max_time = x.loc[x['sort']=='old', 'dt_min_post'].max()\n",
    "    x['in_old'] = x['dt_min_post'] <= max_time\n",
    "    \n",
    "    return x\n",
    "\n",
    "def lists_to_df(post_list, top_list, old_list, n_top):\n",
    "    df = pd.DataFrame(top_list)\n",
    "    df = df.sort_values(['post_id', 'score'], ascending=False).groupby('post_id').head(n_top)\n",
    "    df = df.append(pd.DataFrame(old_list)).merge(pd.DataFrame(post_list), on='post_id')\n",
    "    df.columns = ['post_id', 'comment_id', 'comment_created_utc', 'comment_score', 'top_level',\n",
    "                 'sort', 'sub', 'post_title', 'comments_n', 'post_score', 'post_created_utc']\n",
    "    \n",
    "    ### calculate time since post and if the comment is within the top n oldest\n",
    "    df['dt_min_post'] = ((df['comment_created_utc'] - df['post_created_utc'])/60).clip(0, None)\n",
    "    df['dt_min_cmnt'] = df.groupby('post_id', as_index=False)['comment_created_utc'].transform(lambda x: (x-x.min())/60).clip(0, None)\n",
    "    df = df.groupby('post_id', as_index=False).apply(in_oldest)\n",
    "    \n",
    "    ### sort by sort, creation time; drop dupes to retain only old not in top\n",
    "    df = df.sort_values(['post_id', 'sort', 'comment_created_utc']).reset_index(drop=True)\n",
    "    df = df.drop_duplicates(subset=['post_id', 'comment_id'], keep='last') # df['sort'].unique() = ['old', 'top']\n",
    "    \n",
    "    ### sort by score, assign rank\n",
    "    df = df.sort_values(['post_id', 'comment_score'], ascending=False).reset_index(drop=True)\n",
    "    df['rank'] = df.groupby('post_id', as_index=False)['comment_id'].transform(lambda x: [i+1 for i in range(len(x))])\n",
    "    df.loc[df['sort']=='old', 'rank'] = None\n",
    "    \n",
    "    ### sort only by creation time, assign nth; if not in oldest, replace with predicted n\n",
    "    df = df.sort_values(['post_id', 'comment_created_utc']).reset_index(drop=True)\n",
    "    df['nth'] = df.groupby('post_id', as_index=False)['comment_id'].transform(lambda x: [i+1 for i in range(len(x))])\n",
    "    #df['rate'] = df['nth'] / (df['t_delta_min']+0.5)\n",
    "    df.loc[df['in_old']==False, 'nth'] = None\n",
    "    #df = df.groupby('post_id', as_index=False).apply(interpolate_n)\n",
    "    #df = df.drop(columns=['rate'])\n",
    "    \n",
    "    ### calculate score dominance and \n",
    "    df['comment_score_pct'] = df.groupby(['post_id'], as_index=False)['comment_score'].transform(lambda x: x/x.sum())\n",
    "    df['norm_dom'] = df.groupby(['post_id'], as_index=False)['comment_score'].transform(lambda x: (x/x.sum())/(1/len(x)))\n",
    "    #df['nth_perc'] = (df['nth'] / df['comments_n']).clip(None, 1)\n",
    "    \n",
    "    ### label cleanup, drop unneeded\n",
    "    df.loc[df['sort']=='old', 'sort'] = 'oldest ~500 comments/post'\n",
    "    df.loc[df['sort']=='top', 'sort'] = f'top {n_top} comments/post'\n",
    "                                                                                         \n",
    "    return df\n",
    "\n",
    "\n",
    "def unpickle(sub_name):\n",
    "    post_list = pickle.load(open(f'./data/posts_{sub_name}.pickle', 'rb'))\n",
    "    top_list = pickle.load(open(f'./data/top_{sub_name}.pickle', 'rb'))\n",
    "    old_list = pickle.load(open(f'./data/old_{sub_name}.pickle', 'rb'))\n",
    "    \n",
    "    return post_list, top_list, old_list\n",
    "\n",
    "\n",
    "df_list = [lists_to_df(*unpickle(sub_name), n_top=10) for sub_name in subs]\n",
    "df = pd.concat(df_list)\n",
    "df = df.loc[(df['comments_n'] > 500)]\n",
    "df = df.loc[(df['dt_min_post'] <= 1440)]\n",
    "df = df.groupby('post_id').filter(lambda x: len(x)>250)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb3c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
